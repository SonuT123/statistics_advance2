{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": "                                       STATISTICS ASSIGNMENT ADVANCE-2",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q1: What are the Probability Mass Function (PMF) and Probability Density Function (PDF)? Explain with\n    an example.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Probability Mass Function (PMF) and Probability Density Function (PDF) are mathematical concepts used in probability theory and statistics to describe the probability distribution of a discrete random variable and a continuous random variable, respectively.\n\n1.Probability Mass Function (PMF):\n   >PMF is used for discrete random variables, which are variables that can take on a countable number of distinct values.\n   >The PMF is defined as a function that maps each possible value of the random variable to its probability.\n    \n    \n    Example:-\n    \n    PMF(X = 1) = 1/6\n    PMF(X = 2) = 1/6\n    PMF(X = 3) = 1/6\n    PMF(X = 4) = 1/6\n    PMF(X = 5) = 1/6\n    PMF(X = 6) = 1/6\n    \nthe PMF tells us that each outcome (1, 2, 3, 4, 5, or 6) has an equal probability of 1/6.\n\n2.Probability Density Function (PDF):-\n   >PDF is used for continuous random variables, which are variables that can take on an infinite number of values within a range.\n   >It describes the likelihood of a continuous random variable falling within a specific interval.\n   >The PDF is defined as a function such that the area under the curve over any interval gives the probability that the random \n    variable falls within that interval.\n    \n    Example:-\n      Let's say we have a continuous random variable Y representing the height of people in a population,\n        and it follows a normal distribution with a mean of 170 cm and a standard deviation of 10 cm. The PDF\n        for Y can be represented by the probability density curve:\n\n        f(y) = (1 / (σ√(2π))) * e^(-((y - μ)^2) / (2σ^2))\n\n      Here, μ (mu) is the mean (170), σ (sigma) is the standard deviation (10), and e is Euler's number (approximately 2.71828).\n      This PDF describes how the heights are distributed in the population, with the highest density around the mean value of 170 cm.\n        \n        ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q2: What is Cumulative Density Function (CDF)? Explain with an example. Why CDF is used?",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The cumulative distribution function (CDF) of a random variable is a function that \n       gives the probability that the random variable will take on a \n       value less than or equal to a certain value.\n        \nCDF(x) = P(X ≤ x)\nwhere X is the random variable.\n\nexample of a CDF:\n    \nSuppose you flip a fair coin. The CDF for this event is:\n\nCDF(heads) = P(X ≤ heads) = 1\nCDF(tails) = P(X ≤ tails) = 1\nThis means that the probability of flipping heads is 1 and the probability of flipping tails is 1.\n\n\n example of a CDF:\n        \nSuppose the height of a randomly selected adult male is normally distributed with a mean of 175 cm \n      and a standard deviation of 10 cm. The CDF for this event is:  \n        \n CDF(height) = ∫_{-∞}^height PDF(h) dh\nwhere PDF(h) is the PDF for the height of an adult male.\n\nThis CDF can be used to calculate the probability of a randomly selected adult male being a certain height or less. \n\nFor example,\n\nthe probability of a randomly selected adult male being 180 cm tall or less is:\n    \n\n    CDF(180) = ∫_{-∞}^{180} PDF(h) dh = 0.8413\nThis means that 84.13% of adult males are 180 cm tall or less.\n\nCDFs are used for a variety of purposes, including:\n\nTo calculate the probability of a random variable taking on a certain value or less.\n   To compare the distributions of two or more random variables.\n   To generate random variables from a known distribution.\n   To develop statistical tests and hypotheses.\n   CDFs are a fundamental tool in probability and statistics, and they are used in a wide range of applications,\n   including engineering, finance, and machine learning.\n    \n    ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q3: What are some examples of situations where the normal distribution might be used as a model?\n    Explain how the parameters of the normal distribution relate to the shape of the distribution.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The normal distribution, also known as the Gaussian distribution or bell curve, is a widely used probability distribution\n    in statistics and probability theory. It is characterized by its bell-shaped curve and is often used as a \n    model in various situations where the underlying data exhibits certain characteristics.\n    \nexamples of situations where the normal distribution might be used as a model:\n    \n    \n    1.Height of Individuals:-\n          \n        The heights of a large population of adults often follow a roughly normal distribution. \n        The mean (μ) and standard deviation (σ) of this distribution can provide insights into the average \n        height and the variation around that average.\n        \n     2.IQ Scores:- \n         \n         Intelligence quotient (IQ) scores in a population tend to be normally distributed,\n         with a mean IQ of 100 and a standard deviation of 15. This distribution is useful for\n         understanding the distribution of cognitive abilities in a population.\n            \n     3.Measurement Errors:-\n    \n          In many scientific experiments and measurements,\n          errors are normally distributed. This is particularly important in fields like physics and engineering,\n          where accurate measurements are crucial.\n        \n     4.Stock Prices:-\n          Daily returns of stocks, when observed over a long period, often follow a normal distribution.\n          Financial analysts use the normal distribution to model risk and estimate the probability of extreme price movements.\n            \n      5.Test Scores:\n           In educational testing, scores on standardized tests, such as the SAT or GRE, are often assumed to be normally distributed.\n            This assumption helps set percentile ranks and assess how test-takers performed relative to the general population.\n            \n       6.Biological Phenomena:-\n             Various biological traits, such as birth weights, enzyme activity levels, and reaction times,\n             can be approximated by a normal distribution.\n            \n        7.Quality Control:-\n               In manufacturing processes, the normal distribution is used to model the distribution of product measurements. \n               This helps in setting quality control limits and identifying defective products.\n                \n         The parameters of the normal distribution, mean (μ) and standard deviation (σ),\n              play a crucial role in defining the shape of the distribution:   \n                \n                \n         1.Mean (μ): The mean represents the central location or the peak of the distribution.\n                    It determines where the center of the bell curve is located.\n                    A higher mean shifts the curve to the right, while a lower mean shifts it to the left. \n                \n        \n          2.Standard Deviation (σ): The standard deviation measures the spread or dispersion of data points in the distribution.\n                                   A larger standard deviation results in a wider and flatter curve, indicating greater variability. \n                                  A smaller standard deviation produces a narrower and taller curve, signifying less variability. \n      ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q4: Explain the importance of Normal Distribution. Give a few real-life examples of Normal\n     Distribution.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The normal distribution, also known as the Gaussian distribution or bell curve, holds significant importance in statistics, data analysis, and           various fields for several reasons:\n\n\n1.Central Limit Theorem:-\n      The normal distribution is a fundamental concept in statistics because of the Central Limit Theorem.\n      This theorem states that the distribution of the sample means of many independent,\n      identically distributed random variables will be approximately normally distributed,\n      regardless of the original population distribution. This property allows statisticians \n      to make inferences about populations, even when the population distribution is not normal,\n      by relying on the normal distribution for sample statistics.\n      \n2.Statistical Inference:-\n      Normal distribution forms the basis for many statistical tests and methods, such as hypothesis testing,\n      confidence intervals, and regression analysis. These techniques are widely used in research, quality control,\n      and decision-making processes.\n      \n3.Ease of Interpretation:-\n      The normal distribution has a well-defined shape, with the mean, median, and mode all equal and located at the center\n      of the distribution. This simplicity makes it easy to understand and interpret data. \n      \n4.Modeling Real-Life Phenomena:-\n      Many natural and human-made phenomena exhibit approximately normal distributions,\n      making it a useful model for various situations. Here are a few real-life \n      examples of the normal distribution:  \n      \n      \n      a. Height of Individuals: In a large population, heights often follow a normal distribution.\n         This is useful for understanding the average height and the range of heights in a population.\n         \n      b. IQ Scores: IQ scores are designed to follow a normal distribution with a mean of 100 and a standard deviation of 15.\n         This helps in comparing an individual's intelligence relative to the broader population.\n         \n      c. Errors in Measurements: Measurement errors in scientific experiments are often normally distributed. Scientists \n          and engineers rely on this property to estimate the accuracy of measurements.\n          \n      d. Stock Market Returns: Daily returns of stocks tend to follow a normal distribution over long periods, \n           which is crucial for risk assessment and portfolio management in finance.\n           \n       e. Exam Scores: In educational testing, scores on standardized exams, like the SAT or GRE, are assumed to be normally distributed.\n                  This assumption helps set scoring scales and interpret test results.\n        f. Reaction Times: Reaction times in psychological experiments often approximate a normal distribution.\n           Researchers use this to analyze cognitive processes and human performance.\n           \n    5.Quality Control: Normal distribution is used in quality control to set control limits and identify defects or\n               deviations from the expected in manufacturing and production processes.   \n               ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q5: What is Bernaulli Distribution? Give an Example. What is the difference between Bernoulli\n    Distribution and Binomial Distribution?",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The Bernoulli Distribution is a probability distribution that models a random experiment with two possible outcomes, \n          typically labeled as \"success\" and \"failure.\" It is named after the Swiss mathematician Jacob Bernoulli.\n          The distribution is characterized by a single parameter, often denoted as p, which represents the probability of success.\n        \n        In the Bernoulli Distribution:-\n        \n        >The random variable, often denoted as X, can take one of two values: 1 (for success) or 0 (for failure).\n\n        >The probability mass function (PMF) of the Bernoulli Distribution is given by:\n \n         P(X = 1) = p\n         P(X = 0) = 1 - p\n        \n        Example:\n    Consider a simple experiment of flipping a fair coin, where getting a \"heads\" is considered a success,\n     and getting a \"tails\" is considered a failure. In this case:\n\n    p (the probability of success) is 0.5 because there is a 50% chance of getting a \"heads.\"\n    1 - p (the probability of failure) is also 0.5 because there is a 50% chance of getting a \"tails.\"\n    So, the Bernoulli Distribution for this coin flip experiment can be represented as follows:\n\n    P(X = 1) = 0.5 (probability of success)\n    P(X = 0) = 0.5 (probability of failure)\n    \nBernoulli Distribution:-\n      1.Models a single trial or experiment with two possible outcomes (success or failure).\n      2.Characterized by a single parameter p, which represents the probability of success.\n      3.Random variable X takes values 1 (success) or 0 (failure).\n    \nBinomial Distribution:-\n\n       1.Models the number of successes (or failures) in a fixed number of independent Bernoulli \n         trials (repeated experiments) with the same probability of success (p).\n        \n       2.Characterized by two parameters: n (the number of trials) and p (the probability of success in each trial).\n    \n       3.The random variable Y represents the count of successes in the n trials and can take values from 0 to n.\n        \n        ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q6. Consider a dataset with a mean of 50 and a standard deviation of 10. If we assume that the dataset\n    is normally distributed, what is the probability that a randomly selected observation will be greater\n    than 60? Use the appropriate formula and show your calculations.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "To calculate the probability that a randomly selected observation from a normally distributed \n       dataset with a mean of 50 and a standard deviation of 10 will be greater than 60, \n    \nP(X > 60) = 1 - P(X ≤ 60)\nwhere X is the random variable representing the observation.\n\nTo calculate P(X≤60), we can use the cumulative distribution function (CDF) of the normal distribution.\n        The CDF of the normal distribution is a function that gives the probability that a standard normal\n        variable will be less than or equal to a certain value.\n\nTo calculate the CDF of the normal distribution, we can use a standard normal table or a statistical calculator.\n        For this example, we will use a standard normal table.\n    \nFirst, we need to calculate the z-score of 60. The z-score is calculated as follows:\n\nz = (X - μ) / σ\nwhere μ is the mean of the distribution and σ is the standard deviation of the distribution.\n\nIn this case, the z-score of 60 is:\n\nz = (60 - 50) / 10 = 1.00\n\nWe can then look up the z-score of 1.00 in a standard normal table. This value is 0.8413.\n    This means that the probability that a standard normal variable will be less than or equal to 1.00 is 0.8413.\n    \n    \n\nTherefore, the probability that a randomly selected observation from a normally distributed dataset with \nra mean of 50 and a standard deviation of 10 will be less than or equal to 60 is 0.8413.\n\n     Finally, we can use the following formula to calculate the probability that a randomly selected observation will be greater than 60:\n\nP(X > 60) = 1 - P(X ≤ 60) = 1 - 0.8413 = 0.1587\n       Therefore, the probability that a randomly selected observation from a normally   \n       distributed dataset with a mean of 50 and a standard deviation of 10 will be greater than 60 is 0.1587.\n        \n        \n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q7: Explain uniform Distribution with an example.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "A uniform distribution is a probability distribution in which all outcomes are equally likely.\nIt is also known as a rectangular distribution.\n\nexample of a uniform distribution:\n\n  Suppose you flip a fair coin. The two possible outcomes are heads or tails. Both outcomes are equally likely,\n    so the probability of heads is 1/2 and the probability of tails is 1/2. This is a uniform distribution. \n    \n  xample of a uniform distribution is the height of people in a population. There is no one height\n    that is more likely than any other, so the height of a randomly selected person is uniformly distributed\n    over some range of values.  \n    \n    \nUniform distributions are often used to model situations where all outcomes are equally likely.\n  For example, a random number generator typically generates uniform random numbers. \n  These random numbers can then be used to simulate other types of distributions, such as the normal distribution. \n  \n  \n  mathematical definition of the uniform distribution:\n\n   P(X ≤ x) = (x - a) / (b - a)\n   where X is the random variable, a is the lower bound of the distribution, and b is the upper bound of the distribution.\n\n   The probability density function (PDF) of the uniform distribution is:\n   \n    f(x) = 1 / (b - a)\n    for a≤x≤b.\n\n    The PDF of the uniform distribution is constant over the range of the distribution,\n     which means that all outcomes are equally likely.\n\n   Uniform distributions are a useful tool for modeling a variety of real-world phenomena.\n   For example, the arrival times of customers at a store, the wait times for a bus, and the\n   amount of rain that falls on a given day can all be modeled using uniform distributions.   \n   \n   \n   ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q8: What is the z score? State the importance of the z score.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The z-score, also known as the standard score or standardization, is a statistical measure that quantifies\n      how many standard deviations a data point is away from the mean of a dataset.\n      It is a dimensionless number and is used to standardize data and make meaningful comparisons \n      between data points from different datasets or populations. The formula to calculate the z-score \n      for an individual data point (x) in a dataset with a mean (μ) and standard deviation (σ) is:\n\n   z = (X - μ) / σ\n \n\nHere's what the components of the formula represent:\n\n     X is the value of the specific point\n      μ is the mean of the distribution\n      σ is the standard deviation of the distribution\n    \n    \n     The importance of the z-score lies in several key aspects:\n\n1Standardization: Z-scores transform data into a common scale, allowing for meaningful comparisons.\n  By standardizing data, you remove the original units of measurement, making it easier to analyze\n    and compare data from different sources.\n\n2.Identification of Outliers: Z-scores help identify outliers, which are data points that are significantly \n   different from the rest of the data. Outliers often have z-scores that are much greater or much smaller than zero.\n\n3.Probability and Percentiles: Z-scores are used in statistical tables and calculators to find probabilities \n  associated with a specific value in a normal distribution. They are also used to calculate percentiles,\n  helping to understand where a data point ranks relative to the rest of the data.\n\n4.Quality Control: In quality control and manufacturing, z-scores are used to set tolerance limits and determine whether \n   a product or process is within acceptable quality standards.\n\n5.Hypothesis Testing: In hypothesis testing, z-scores play a critical role.\n   For example, they are used to calculate test statistics in hypothesis tests and assess the significance of research findings.\n\n6.Standardizing Variables: When dealing with multiple variables in statistical analysis,\n    standardizing them using z-scores can help give each variable equal weight and make it easier \n    to compare their contributions to an analysis.\n\n7.ata Exploration: Z-scores are useful for data exploration and visualization.\n           They can help reveal patterns and relationships within a dataset.\n\n In summary, the z-score is a valuable statistical tool that standardizes data,\n    making it easier to analyze, compare, and interpret. It provides a common scale \n    for data points, aids in identifying outliers, and is widely used in various statistical analyses,\n    hypothesis testing, quality control, and data exploration.\n\n\n\n\n",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q9: What is Central Limit Theorem? State the significance of the Central Limit Theorem.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "The Central Limit Theorem (CLT) is a fundamental concept in probability theory and statistics.\n     It states that when you have a sufficiently large sample size drawn from any population with\n     a finite mean (μ) and finite variance (σ^2), the distribution of the sample means will be \n      approximately normally distributed, regardless of the shape of the original population distribution.\n      In other words, as the sample size increases, the sampling distribution of the sample mean approaches\n       a normal distribution.\n    \n    \nKey points about the Central Limit Theorem:\n    \n       1.Large Sample Size: The CLT applies when the sample size is sufficiently large,\n        typically considered to be around n ≥ 30. However, the larger the sample size, \n         the closer the sampling distribution of the mean is to a normal distribution. \n            \n       2.Independence: The individual observations in the sample should be independent of each other\n    \n       3.Finite Mean and Variance: The population from which the samples are drawn must have a finite mean\n          (μ) and a finite variance (σ^2).\n            \n    Significance of the Central Limit Theorem:-\n    \n     1.Normal Distribution Approximation: The CLT is significant because it allows statisticians to make inferences\n        about population parameters, even when the population distribution is not known or is not normally distributed.\n        It assures us that the distribution of sample means will be approximately normal,\n        making it easier to perform statistical analyses.\n        \n    2.Hypothesis Testing: The CLT is crucial for hypothesis testing. It enables the use of statistical tests like \n         the z-test and t-test, which assume that the sampling distribution of the sample mean is approximately normal.\n          This simplifies the testing process and allows for more robust conclusions.\n            \n     3.Confidence Intervals:The CLT is used in constructing confidence intervals for population parameters, such as the mean.\n           It ensures that these intervals have desirable properties, even when the population distribution is not known.\n        \n      4.Statistical Estimation: It is used in estimating population parameters. For example, when estimating the mean of a population,\n            the sample mean is used as a point estimate, and the CLT helps in understanding the distribution of possible sample means.\n        \n      5.Quality Control: In manufacturing and quality control, the CLT is used to assess the quality of products by sampling and\n                analyzing the means or other statistics of the samples.\n        \n      6.Large-Scale Surveys: When conducting large-scale surveys or opinion polls, the CLT allows statisticians to make inferences\n             about the population using data from a reasonably sized sample.\n        \n        ",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "Q10: State the assumptions of the Central Limit Theorem.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "The Central Limit Theorem (CLT) is a fundamental concept in statistics that provides valuable insights into \n    the sampling distribution of the sample mean. To apply the CLT successfully, certain assumptions must be met.\n    These assumptions include:\n    \n   1.Independence: The observations or data points in the sample must be independent of each other. \n            This means that the value of one data point should not influence or be related to the value \n            of any other data point in the sample \n            \n    2.Random Sampling: The sample should be drawn randomly from the population of interest. \n      Random sampling ensures that each member of the population has an equal chance of being  \n       included in the sample, reducing potential bias.\n    \n    3.Finite Population or Infinite Population: The CLT technically applies to both finite and infinite populations, \n         but the assumptions and implications may differ slightly. For finite populations, sampling is typically \n        done without replacement, and the sample size should be less than 10% of the population size to ensure\n        independence among samples.\n        \n    4.Finite Mean and Variance: The population from which the samples are drawn should have a finite mean (μ) and a finite variance (σ^2).\n           This is a crucial assumption because it ensures that the population distribution has well-defined parameters.\n        \n    5.Sufficiently Large Sample Size: The CLT becomes more reliable as the sample size (n) increases. While there is no fixed minimum\n           sample size requirement, a common rule of thumb is that n ≥ 30 is often considered sufficiently large for the CLT to apply.\n            However, in some cases, smaller sample sizes may still provide reasonable approximations to a normal distribution.\n            \n            ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}